{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 680 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from keras.optimizers import SGD, Nadam, RMSprop\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "import config\n",
    "\n",
    "from utils.dataset.data_generator import DataGenerator\n",
    "from models.cnn3 import cnn, cnn_regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "decay = 0.001\n",
    "l1 = 0.00001\n",
    "l2 = 0.00001\n",
    "dropout = 0.5\n",
    "n_epochs = 500\n",
    "batch_size = 32\n",
    "input_shape = (140, 140, 3)\n",
    "\n",
    "name = 'cnn_140_rgb_lr_%f_decay_%f_sgd_he_normal__l1_%f_l2_%f_dropout_%f_r' % (lr, decay, l1, l2, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 140, 140, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 140, 140, 128) 18944       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 140, 140, 128) 0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 70, 70, 128)   0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 70, 70, 64)    204864      maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 70, 70, 64)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 35, 35, 64)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 35, 35, 64)    36928       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 35, 35, 64)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 17, 17, 64)    0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 18496)         0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          18940928    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 1024)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1024)          1049600     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 1024)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 512)           524800      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             1026        dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 2)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 20777090\n",
      "____________________________________________________________________________________________________\n",
      "compiling model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('loading model...')\n",
    "# model = cnn(input_shape=input_shape, init='he_normal')\n",
    "model = cnn_regularized(input_shape=input_shape, init='he_normal', l1=l1, l2=l2)\n",
    "model.summary()\n",
    "\n",
    "optimizer = SGD(lr=lr, clipnorm=4., nesterov=True, decay=decay)\n",
    "# optimizer = Nadam(lr=lr)\n",
    "# optimizer = RMSprop(lr=lr)\n",
    "\n",
    "print('compiling model...')\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print('done.')\n",
    "\n",
    "csv_logger = CSVLogger('%s_training.log' % name)\n",
    "best_model_checkpointer = ModelCheckpoint(filepath=(\"./%s_training_weights_best.hdf5\" % name), verbose=1,\n",
    "                                          save_best_only=True)\n",
    "\n",
    "current_model_checkpointer = ModelCheckpoint(filepath=(\"./%s_training_weights_current.hdf5\" % name), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data generators...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('Initializing data generators...')\n",
    "train_data_gen = DataGenerator(dataset_file=config.train_data_file, batch_size=batch_size)\n",
    "validation_data_gen = DataGenerator(dataset_file=config.validation_data_file, batch_size=batch_size)\n",
    "test_data_gen = DataGenerator(dataset_file=config.test_data_file, batch_size=batch_size)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Epoch 1/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.8059 - acc: 0.6562Epoch 00000: val_loss improved from inf to 0.48095, saving model to ./cnn_140_rgb_lr_0.100000_decay_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 355s - loss: 2.8058 - acc: 0.6562 - val_loss: 0.4809 - val_acc: 0.7449\n",
      "Epoch 2/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.6110 - acc: 0.7498Epoch 00001: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 2.6110 - acc: 0.7497 - val_loss: 0.4885 - val_acc: 0.7619\n",
      "Epoch 3/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.4028 - acc: 0.8720Epoch 00002: val_loss improved from 0.48095 to 0.30334, saving model to ./cnn_140_rgb_lr_0.100000_decay_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 359s - loss: 2.4027 - acc: 0.8720 - val_loss: 0.3033 - val_acc: 0.8776\n",
      "Epoch 4/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.3296 - acc: 0.8904Epoch 00003: val_loss improved from 0.30334 to 0.24943, saving model to ./cnn_140_rgb_lr_0.100000_decay_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 360s - loss: 2.3293 - acc: 0.8906 - val_loss: 0.2494 - val_acc: 0.9025\n",
      "Epoch 5/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.2733 - acc: 0.8956Epoch 00004: val_loss improved from 0.24943 to 0.24620, saving model to ./cnn_140_rgb_lr_0.100000_decay_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 358s - loss: 2.2733 - acc: 0.8956 - val_loss: 0.2462 - val_acc: 0.9021\n",
      "Epoch 6/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.2368 - acc: 0.9035Epoch 00005: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 2.2372 - acc: 0.9034 - val_loss: 0.2856 - val_acc: 0.8847\n",
      "Epoch 7/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.2032 - acc: 0.9080Epoch 00006: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 2.2033 - acc: 0.9078 - val_loss: 0.2500 - val_acc: 0.8988\n",
      "Epoch 8/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.1767 - acc: 0.9130Epoch 00007: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 2.1766 - acc: 0.9130 - val_loss: 0.2467 - val_acc: 0.8970\n",
      "Epoch 9/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.1490 - acc: 0.9155Epoch 00008: val_loss improved from 0.24620 to 0.22698, saving model to ./cnn_140_rgb_lr_0.100000_decay_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 359s - loss: 2.1492 - acc: 0.9154 - val_loss: 0.2270 - val_acc: 0.9162\n",
      "Epoch 10/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.1287 - acc: 0.9183Epoch 00009: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 2.1286 - acc: 0.9183 - val_loss: 0.2323 - val_acc: 0.9096\n",
      "Epoch 11/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.1064 - acc: 0.9205Epoch 00010: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 2.1063 - acc: 0.9206 - val_loss: 0.2288 - val_acc: 0.9100\n",
      "Epoch 12/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0878 - acc: 0.9226Epoch 00011: val_loss improved from 0.22698 to 0.22173, saving model to ./cnn_140_rgb_lr_0.100000_decay_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 359s - loss: 2.0878 - acc: 0.9224 - val_loss: 0.2217 - val_acc: 0.9164\n",
      "Epoch 13/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0704 - acc: 0.9280Epoch 00012: val_loss improved from 0.22173 to 0.21532, saving model to ./cnn_140_rgb_lr_0.100000_decay_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 357s - loss: 2.0702 - acc: 0.9280 - val_loss: 0.2153 - val_acc: 0.9192\n",
      "Epoch 14/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0521 - acc: 0.9293Epoch 00013: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 2.0520 - acc: 0.9293 - val_loss: 0.2177 - val_acc: 0.9181\n",
      "Epoch 15/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0382 - acc: 0.9322Epoch 00014: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 2.0381 - acc: 0.9322 - val_loss: 0.2229 - val_acc: 0.9162\n",
      "Epoch 16/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0215 - acc: 0.9330Epoch 00015: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 2.0214 - acc: 0.9330 - val_loss: 0.2209 - val_acc: 0.9190\n",
      "Epoch 17/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0082 - acc: 0.9359Epoch 00016: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 2.0080 - acc: 0.9358 - val_loss: 0.2175 - val_acc: 0.9230\n",
      "Epoch 18/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9936 - acc: 0.9397Epoch 00017: val_loss improved from 0.21532 to 0.20964, saving model to ./cnn_140_rgb_lr_0.100000_decay_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 358s - loss: 1.9934 - acc: 0.9397 - val_loss: 0.2096 - val_acc: 0.9261\n",
      "Epoch 19/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9820 - acc: 0.9386Epoch 00018: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9817 - acc: 0.9387 - val_loss: 0.2125 - val_acc: 0.9241\n",
      "Epoch 20/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9630 - acc: 0.9443Epoch 00019: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9628 - acc: 0.9444 - val_loss: 0.2135 - val_acc: 0.9265\n",
      "Epoch 21/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9529 - acc: 0.9442Epoch 00020: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.9527 - acc: 0.9442 - val_loss: 0.2117 - val_acc: 0.9241\n",
      "Epoch 22/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9414 - acc: 0.9477Epoch 00021: val_loss improved from 0.20964 to 0.20949, saving model to ./cnn_140_rgb_lr_0.100000_decay_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 358s - loss: 1.9411 - acc: 0.9479 - val_loss: 0.2095 - val_acc: 0.9278\n",
      "Epoch 23/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9271 - acc: 0.9515Epoch 00022: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9269 - acc: 0.9516 - val_loss: 0.2210 - val_acc: 0.9258\n",
      "Epoch 24/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9166 - acc: 0.9543Epoch 00023: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.9163 - acc: 0.9545 - val_loss: 0.2175 - val_acc: 0.9272\n",
      "Epoch 25/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9037 - acc: 0.9560Epoch 00024: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.9035 - acc: 0.9561 - val_loss: 0.2172 - val_acc: 0.9254\n",
      "Epoch 26/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8934 - acc: 0.9570Epoch 00025: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8934 - acc: 0.9571 - val_loss: 0.2266 - val_acc: 0.9247\n",
      "Epoch 27/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8819 - acc: 0.9620Epoch 00026: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8817 - acc: 0.9621 - val_loss: 0.2249 - val_acc: 0.9258\n",
      "Epoch 28/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8716 - acc: 0.9620Epoch 00027: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8714 - acc: 0.9621 - val_loss: 0.2291 - val_acc: 0.9243\n",
      "Epoch 29/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8607 - acc: 0.9638Epoch 00028: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8605 - acc: 0.9638 - val_loss: 0.2274 - val_acc: 0.9309\n",
      "Epoch 30/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8522 - acc: 0.9651Epoch 00029: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8520 - acc: 0.9652 - val_loss: 0.2270 - val_acc: 0.9302\n",
      "Epoch 31/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8440 - acc: 0.9665Epoch 00030: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8437 - acc: 0.9666 - val_loss: 0.2324 - val_acc: 0.9313\n",
      "Epoch 32/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8304 - acc: 0.9709Epoch 00031: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8302 - acc: 0.9710 - val_loss: 0.2351 - val_acc: 0.9276\n",
      "Epoch 33/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8241 - acc: 0.9710Epoch 00032: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8239 - acc: 0.9711 - val_loss: 0.2348 - val_acc: 0.9274\n",
      "Epoch 34/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8147 - acc: 0.9746Epoch 00033: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8145 - acc: 0.9747 - val_loss: 0.2396 - val_acc: 0.9305\n",
      "Epoch 35/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.8094 - acc: 0.9733Epoch 00034: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.8092 - acc: 0.9734 - val_loss: 0.2424 - val_acc: 0.9311\n",
      "Epoch 36/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7999 - acc: 0.9762Epoch 00035: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7997 - acc: 0.9762 - val_loss: 0.2503 - val_acc: 0.9300\n",
      "Epoch 37/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7892 - acc: 0.9796Epoch 00036: val_loss did not improve\n",
      "10496/10496 [==============================] - 359s - loss: 1.7891 - acc: 0.9797 - val_loss: 0.2416 - val_acc: 0.9311\n",
      "Epoch 38/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7839 - acc: 0.9791Epoch 00037: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7839 - acc: 0.9790 - val_loss: 0.2592 - val_acc: 0.9294\n",
      "Epoch 39/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7773 - acc: 0.9807Epoch 00038: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7771 - acc: 0.9808 - val_loss: 0.2525 - val_acc: 0.9322\n",
      "Epoch 40/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7722 - acc: 0.9802Epoch 00039: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7720 - acc: 0.9802 - val_loss: 0.2515 - val_acc: 0.9331\n",
      "Epoch 41/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7643 - acc: 0.9829Epoch 00040: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7642 - acc: 0.9829 - val_loss: 0.2687 - val_acc: 0.9313\n",
      "Epoch 42/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7612 - acc: 0.9817Epoch 00041: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7610 - acc: 0.9817 - val_loss: 0.2683 - val_acc: 0.9305\n",
      "Epoch 43/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7545 - acc: 0.9834Epoch 00042: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.7544 - acc: 0.9834 - val_loss: 0.2766 - val_acc: 0.9313\n",
      "Epoch 44/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7483 - acc: 0.9833Epoch 00043: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7482 - acc: 0.9833 - val_loss: 0.2684 - val_acc: 0.9309\n",
      "Epoch 45/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7429 - acc: 0.9837Epoch 00044: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7428 - acc: 0.9837 - val_loss: 0.2659 - val_acc: 0.9316\n",
      "Epoch 46/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7376 - acc: 0.9854Epoch 00045: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7375 - acc: 0.9855 - val_loss: 0.2776 - val_acc: 0.9318\n",
      "Epoch 47/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7315 - acc: 0.9864Epoch 00046: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7314 - acc: 0.9865 - val_loss: 0.2616 - val_acc: 0.9327\n",
      "Epoch 48/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7249 - acc: 0.9872Epoch 00047: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7248 - acc: 0.9872 - val_loss: 0.2701 - val_acc: 0.9309\n",
      "Epoch 49/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7215 - acc: 0.9883Epoch 00048: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7214 - acc: 0.9884 - val_loss: 0.2830 - val_acc: 0.9283\n",
      "Epoch 50/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7149 - acc: 0.9882Epoch 00049: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7148 - acc: 0.9883 - val_loss: 0.2903 - val_acc: 0.9300\n",
      "Epoch 51/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7073 - acc: 0.9915Epoch 00050: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7073 - acc: 0.9915 - val_loss: 0.3132 - val_acc: 0.9313\n",
      "Epoch 52/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7072 - acc: 0.9892Epoch 00051: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7071 - acc: 0.9893 - val_loss: 0.2897 - val_acc: 0.9327\n",
      "Epoch 53/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.7030 - acc: 0.9896Epoch 00052: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.7030 - acc: 0.9897 - val_loss: 0.2944 - val_acc: 0.9340\n",
      "Epoch 54/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6987 - acc: 0.9910Epoch 00053: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6986 - acc: 0.9910 - val_loss: 0.3109 - val_acc: 0.9313\n",
      "Epoch 55/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6950 - acc: 0.9915Epoch 00054: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6949 - acc: 0.9915 - val_loss: 0.2962 - val_acc: 0.9309\n",
      "Epoch 56/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6900 - acc: 0.9899Epoch 00055: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.6899 - acc: 0.9899 - val_loss: 0.3105 - val_acc: 0.9296\n",
      "Epoch 57/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6866 - acc: 0.9909Epoch 00056: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6865 - acc: 0.9909 - val_loss: 0.2949 - val_acc: 0.9331\n",
      "Epoch 58/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6818 - acc: 0.9929Epoch 00057: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.6818 - acc: 0.9929 - val_loss: 0.2985 - val_acc: 0.9357\n",
      "Epoch 59/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6776 - acc: 0.9928Epoch 00058: val_loss did not improve\n",
      "10496/10496 [==============================] - 359s - loss: 1.6776 - acc: 0.9929 - val_loss: 0.3056 - val_acc: 0.9349\n",
      "Epoch 60/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6735 - acc: 0.9926Epoch 00059: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6734 - acc: 0.9927 - val_loss: 0.3240 - val_acc: 0.9375\n",
      "Epoch 61/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6743 - acc: 0.9923Epoch 00060: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.6742 - acc: 0.9923 - val_loss: 0.3155 - val_acc: 0.9318\n",
      "Epoch 62/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6690 - acc: 0.9924Epoch 00061: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6689 - acc: 0.9924 - val_loss: 0.3316 - val_acc: 0.9305\n",
      "Epoch 63/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6640 - acc: 0.9933Epoch 00062: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6640 - acc: 0.9933 - val_loss: 0.3263 - val_acc: 0.9322\n",
      "Epoch 64/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6603 - acc: 0.9932Epoch 00063: val_loss did not improve\n",
      "10496/10496 [==============================] - 359s - loss: 1.6603 - acc: 0.9932 - val_loss: 0.3359 - val_acc: 0.9309\n",
      "Epoch 65/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6582 - acc: 0.9943Epoch 00064: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6582 - acc: 0.9943 - val_loss: 0.3313 - val_acc: 0.9318\n",
      "Epoch 66/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6532 - acc: 0.9938Epoch 00065: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6531 - acc: 0.9938 - val_loss: 0.3311 - val_acc: 0.9335\n",
      "Epoch 67/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6517 - acc: 0.9938Epoch 00066: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6516 - acc: 0.9938 - val_loss: 0.3348 - val_acc: 0.9353\n",
      "Epoch 68/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6493 - acc: 0.9941Epoch 00067: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6492 - acc: 0.9941 - val_loss: 0.3352 - val_acc: 0.9327\n",
      "Epoch 69/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6478 - acc: 0.9947Epoch 00068: val_loss did not improve\n",
      "10496/10496 [==============================] - 359s - loss: 1.6477 - acc: 0.9948 - val_loss: 0.3549 - val_acc: 0.9327\n",
      "Epoch 70/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6423 - acc: 0.9955Epoch 00069: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.6423 - acc: 0.9955 - val_loss: 0.3352 - val_acc: 0.9340\n",
      "Epoch 71/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6411 - acc: 0.9952Epoch 00070: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6410 - acc: 0.9952 - val_loss: 0.3494 - val_acc: 0.9349\n",
      "Epoch 72/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6368 - acc: 0.9955Epoch 00071: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6368 - acc: 0.9955 - val_loss: 0.3598 - val_acc: 0.9335\n",
      "Epoch 73/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6358 - acc: 0.9947Epoch 00072: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6357 - acc: 0.9948 - val_loss: 0.3638 - val_acc: 0.9331\n",
      "Epoch 74/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6323 - acc: 0.9953Epoch 00073: val_loss did not improve\n",
      "10496/10496 [==============================] - 359s - loss: 1.6323 - acc: 0.9953 - val_loss: 0.3654 - val_acc: 0.9305\n",
      "Epoch 75/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6320 - acc: 0.9953Epoch 00074: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6320 - acc: 0.9953 - val_loss: 0.3647 - val_acc: 0.9331\n",
      "Epoch 76/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6270 - acc: 0.9963Epoch 00075: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6270 - acc: 0.9963 - val_loss: 0.3634 - val_acc: 0.9322\n",
      "Epoch 77/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6255 - acc: 0.9948Epoch 00076: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6254 - acc: 0.9949 - val_loss: 0.3591 - val_acc: 0.9318\n",
      "Epoch 78/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6232 - acc: 0.9961Epoch 00077: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6232 - acc: 0.9961 - val_loss: 0.3631 - val_acc: 0.9322\n",
      "Epoch 79/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6198 - acc: 0.9965Epoch 00078: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6198 - acc: 0.9965 - val_loss: 0.3704 - val_acc: 0.9313\n",
      "Epoch 80/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6189 - acc: 0.9960Epoch 00079: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6188 - acc: 0.9960 - val_loss: 0.3664 - val_acc: 0.9305\n",
      "Epoch 81/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6148 - acc: 0.9971Epoch 00080: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.6148 - acc: 0.9971 - val_loss: 0.3917 - val_acc: 0.9296\n",
      "Epoch 82/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6140 - acc: 0.9960Epoch 00081: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6140 - acc: 0.9960 - val_loss: 0.3701 - val_acc: 0.9327\n",
      "Epoch 83/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6139 - acc: 0.9955Epoch 00082: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6140 - acc: 0.9954 - val_loss: 0.3840 - val_acc: 0.9327\n",
      "Epoch 84/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6107 - acc: 0.9957Epoch 00083: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6107 - acc: 0.9957 - val_loss: 0.3797 - val_acc: 0.9300\n",
      "Epoch 85/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6076 - acc: 0.9968Epoch 00084: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6076 - acc: 0.9969 - val_loss: 0.3894 - val_acc: 0.9313\n",
      "Epoch 86/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6060 - acc: 0.9968Epoch 00085: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6060 - acc: 0.9968 - val_loss: 0.3959 - val_acc: 0.9300\n",
      "Epoch 87/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6039 - acc: 0.9965Epoch 00086: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6038 - acc: 0.9965 - val_loss: 0.3895 - val_acc: 0.9291\n",
      "Epoch 88/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5996 - acc: 0.9977Epoch 00087: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.5995 - acc: 0.9977 - val_loss: 0.3908 - val_acc: 0.9291\n",
      "Epoch 89/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.6006 - acc: 0.9966Epoch 00088: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.6005 - acc: 0.9966 - val_loss: 0.3934 - val_acc: 0.9305\n",
      "Epoch 90/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5978 - acc: 0.9967Epoch 00089: val_loss did not improve\n",
      "10496/10496 [==============================] - 359s - loss: 1.5977 - acc: 0.9967 - val_loss: 0.3865 - val_acc: 0.9309\n",
      "Epoch 91/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5955 - acc: 0.9970Epoch 00090: val_loss did not improve\n",
      "10496/10496 [==============================] - 360s - loss: 1.5955 - acc: 0.9970 - val_loss: 0.3922 - val_acc: 0.9318\n",
      "Epoch 92/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5935 - acc: 0.9973Epoch 00091: val_loss did not improve\n",
      "10496/10496 [==============================] - 359s - loss: 1.5935 - acc: 0.9973 - val_loss: 0.3983 - val_acc: 0.9318\n",
      "Epoch 93/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5907 - acc: 0.9972Epoch 00092: val_loss did not improve\n",
      "10496/10496 [==============================] - 360s - loss: 1.5907 - acc: 0.9972 - val_loss: 0.4099 - val_acc: 0.9327\n",
      "Epoch 94/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5899 - acc: 0.9972Epoch 00093: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.5898 - acc: 0.9972 - val_loss: 0.4004 - val_acc: 0.9300\n",
      "Epoch 95/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5883 - acc: 0.9976Epoch 00094: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.5883 - acc: 0.9976 - val_loss: 0.4048 - val_acc: 0.9287\n",
      "Epoch 96/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5874 - acc: 0.9967Epoch 00095: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.5874 - acc: 0.9967 - val_loss: 0.4025 - val_acc: 0.9291\n",
      "Epoch 97/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5850 - acc: 0.9972Epoch 00096: val_loss did not improve\n",
      "10496/10496 [==============================] - 358s - loss: 1.5850 - acc: 0.9972 - val_loss: 0.3957 - val_acc: 0.9291\n",
      "Epoch 98/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5830 - acc: 0.9975Epoch 00097: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.5830 - acc: 0.9975 - val_loss: 0.3947 - val_acc: 0.9287\n",
      "Epoch 99/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5812 - acc: 0.9974Epoch 00098: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.5812 - acc: 0.9974 - val_loss: 0.3934 - val_acc: 0.9327\n",
      "Epoch 100/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.5802 - acc: 0.9975Epoch 00099: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.5802 - acc: 0.9975 - val_loss: 0.4062 - val_acc: 0.9300\n",
      "Epoch 101/500\n",
      " 3968/10496 [==========>...................] - ETA: 214s - loss: 1.5792 - acc: 0.9972"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2c62c6de5cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                               \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                               callbacks=[csv_logger, best_model_checkpointer, current_model_checkpointer])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanuj/.tools/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1460\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1462\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanuj/.tools/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanuj/.tools/anaconda3/envs/py27/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanuj/.tools/anaconda3/envs/py27/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Fitting model...')\n",
    "history = model.fit_generator(train_data_gen,\n",
    "                              nb_epoch=n_epochs,\n",
    "                              samples_per_epoch=train_data_gen.n_batches * batch_size,\n",
    "                              validation_data=validation_data_gen,\n",
    "                              nb_val_samples=validation_data_gen.n_samples,\n",
    "                              verbose=1,\n",
    "                              callbacks=[csv_logger, best_model_checkpointer, current_model_checkpointer])\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "done.\n",
      "Test score: 0.410274893211\n",
      "Test accuracy: 0.931338028169\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating model...')\n",
    "score = model.evaluate_generator(test_data_gen, val_samples=test_data_gen.n_samples)\n",
    "print('done.')\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
