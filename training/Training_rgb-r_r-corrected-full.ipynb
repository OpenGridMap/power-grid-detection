{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 1: GeForce GTX 680 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from keras.optimizers import SGD, Nadam, RMSprop\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "import config\n",
    "\n",
    "from utils.dataset.data_generator import DataGenerator\n",
    "from models.cnn3 import cnn, cnn_regularized\n",
    "from utils.training.callbacks import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr=0.005\n",
    "l1 = 0.00001\n",
    "l2 = 0.00001\n",
    "dropout = 0.5\n",
    "n_epochs=100\n",
    "batch_size=32\n",
    "input_shape=(140, 140, 3)\n",
    "weights='cnn_140_rgb_corrected_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5'\n",
    "\n",
    "name = 'cnn_140_rgb_corrected_full_lr_%f_sgd_he_normal__l1_%f_l2_%f_dropout_%f_r' % (lr, l1, l2, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 140, 140, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 140, 140, 128) 18944       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 140, 140, 128) 0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 70, 70, 128)   0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 70, 70, 64)    204864      maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 70, 70, 64)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 35, 35, 64)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 35, 35, 64)    36928       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 35, 35, 64)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 17, 17, 64)    0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 18496)         0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          18940928    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 1024)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1024)          1049600     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 1024)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 512)           524800      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             1026        dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 2)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 20777090\n",
      "____________________________________________________________________________________________________\n",
      "compiling model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('loading model...')\n",
    "# model = cnn(input_shape=input_shape, init='he_normal')\n",
    "model = cnn_regularized(input_shape=input_shape, init='he_normal', l1=l1, l2=l2, weights=weights)\n",
    "model.summary()\n",
    "\n",
    "optimizer = SGD(lr=lr, clipnorm=4., nesterov=True)\n",
    "# optimizer = Nadam(lr=lr)\n",
    "# optimizer = RMSprop(lr=lr)\n",
    "\n",
    "print('compiling model...')\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print('done.')\n",
    "\n",
    "logger = Logger('%s_training_batch.log' % name, append=True)\n",
    "csv_logger = CSVLogger('%s_training.log' % name, append=True)\n",
    "best_model_checkpointer = ModelCheckpoint(filepath=(\"./%s_training_weights_best.hdf5\" % name), verbose=1,\n",
    "                                          save_best_only=True)\n",
    "\n",
    "current_model_checkpointer = ModelCheckpoint(filepath=(\"./%s_training_weights_current.hdf5\" % name), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data generators...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('Initializing data generators...')\n",
    "train_set_file = '/home/tanuj/Workspace/power-grid-detection/dataset/corrected/19/train_data.csv'\n",
    "validation_set_file = '/home/tanuj/Workspace/power-grid-detection/dataset/corrected/19/validation_data.csv'\n",
    "test_set_file = '/home/tanuj/Workspace/power-grid-detection/dataset/corrected/19/test_data.csv'\n",
    "\n",
    "train_data_gen = DataGenerator(dataset_file=train_set_file, batch_size=batch_size)\n",
    "validation_data_gen = DataGenerator(dataset_file=validation_set_file, batch_size=batch_size)\n",
    "test_data_gen = DataGenerator(dataset_file=test_set_file, batch_size=batch_size)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Epoch 1/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.9500 - acc: 0.9871Epoch 00000: val_loss improved from inf to 0.02843, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2142s - loss: 1.9501 - acc: 0.9871 - val_loss: 0.0284 - val_acc: 0.9904\n",
      "Epoch 2/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.9272 - acc: 0.9880Epoch 00001: val_loss improved from 0.02843 to 0.02736, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2148s - loss: 1.9272 - acc: 0.9880 - val_loss: 0.0274 - val_acc: 0.9908\n",
      "Epoch 3/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.9071 - acc: 0.9884Epoch 00002: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 1.9072 - acc: 0.9884 - val_loss: 0.0278 - val_acc: 0.9907\n",
      "Epoch 4/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.8867 - acc: 0.9892Epoch 00003: val_loss improved from 0.02736 to 0.02536, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2150s - loss: 1.8868 - acc: 0.9892 - val_loss: 0.0254 - val_acc: 0.9915\n",
      "Epoch 5/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.8672 - acc: 0.9894Epoch 00004: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.8673 - acc: 0.9893 - val_loss: 0.0273 - val_acc: 0.9910\n",
      "Epoch 6/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.8480 - acc: 0.9899Epoch 00005: val_loss improved from 0.02536 to 0.02473, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2147s - loss: 1.8480 - acc: 0.9899 - val_loss: 0.0247 - val_acc: 0.9917\n",
      "Epoch 7/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.8284 - acc: 0.9903Epoch 00006: val_loss improved from 0.02473 to 0.02434, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2148s - loss: 1.8284 - acc: 0.9902 - val_loss: 0.0243 - val_acc: 0.9917\n",
      "Epoch 8/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.8091 - acc: 0.9906Epoch 00007: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 1.8091 - acc: 0.9906 - val_loss: 0.0244 - val_acc: 0.9919\n",
      "Epoch 9/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.7908 - acc: 0.9907Epoch 00008: val_loss improved from 0.02434 to 0.02397, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2146s - loss: 1.7908 - acc: 0.9907 - val_loss: 0.0240 - val_acc: 0.9922\n",
      "Epoch 10/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.7719 - acc: 0.9911Epoch 00009: val_loss improved from 0.02397 to 0.02349, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2146s - loss: 1.7720 - acc: 0.9911 - val_loss: 0.0235 - val_acc: 0.9922\n",
      "Epoch 11/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.7533 - acc: 0.9915Epoch 00010: val_loss did not improve\n",
      "65504/65504 [==============================] - 2148s - loss: 1.7533 - acc: 0.9915 - val_loss: 0.0241 - val_acc: 0.9920\n",
      "Epoch 12/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.7356 - acc: 0.9917Epoch 00011: val_loss improved from 0.02349 to 0.02287, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2147s - loss: 1.7356 - acc: 0.9917 - val_loss: 0.0229 - val_acc: 0.9924\n",
      "Epoch 13/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.7178 - acc: 0.9917Epoch 00012: val_loss improved from 0.02287 to 0.02270, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2149s - loss: 1.7178 - acc: 0.9916 - val_loss: 0.0227 - val_acc: 0.9922\n",
      "Epoch 14/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.6997 - acc: 0.9920Epoch 00013: val_loss did not improve\n",
      "65504/65504 [==============================] - 2148s - loss: 1.6997 - acc: 0.9920 - val_loss: 0.0230 - val_acc: 0.9922\n",
      "Epoch 15/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.6826 - acc: 0.9920Epoch 00014: val_loss improved from 0.02270 to 0.02174, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2151s - loss: 1.6826 - acc: 0.9920 - val_loss: 0.0217 - val_acc: 0.9926\n",
      "Epoch 16/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.6648 - acc: 0.9922Epoch 00015: val_loss improved from 0.02174 to 0.02062, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2146s - loss: 1.6649 - acc: 0.9922 - val_loss: 0.0206 - val_acc: 0.9928\n",
      "Epoch 17/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.6476 - acc: 0.9926Epoch 00016: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 1.6476 - acc: 0.9926 - val_loss: 0.0217 - val_acc: 0.9927\n",
      "Epoch 18/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.6309 - acc: 0.9924Epoch 00017: val_loss did not improve\n",
      "65504/65504 [==============================] - 2148s - loss: 1.6310 - acc: 0.9924 - val_loss: 0.0217 - val_acc: 0.9928\n",
      "Epoch 19/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.6138 - acc: 0.9925Epoch 00018: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 1.6138 - acc: 0.9925 - val_loss: 0.0210 - val_acc: 0.9935\n",
      "Epoch 20/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.5978 - acc: 0.9928Epoch 00019: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.5978 - acc: 0.9928 - val_loss: 0.0230 - val_acc: 0.9925\n",
      "Epoch 21/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.5811 - acc: 0.9930Epoch 00020: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.5811 - acc: 0.9930 - val_loss: 0.0239 - val_acc: 0.9921\n",
      "Epoch 22/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.5642 - acc: 0.9932Epoch 00021: val_loss did not improve\n",
      "65504/65504 [==============================] - 2147s - loss: 1.5642 - acc: 0.9932 - val_loss: 0.0207 - val_acc: 0.9936\n",
      "Epoch 23/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.5484 - acc: 0.9929Epoch 00022: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 1.5484 - acc: 0.9929 - val_loss: 0.0240 - val_acc: 0.9921\n",
      "Epoch 24/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.5323 - acc: 0.9934Epoch 00023: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.5324 - acc: 0.9934 - val_loss: 0.0261 - val_acc: 0.9911\n",
      "Epoch 25/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.5164 - acc: 0.9934Epoch 00024: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 1.5165 - acc: 0.9934 - val_loss: 0.0222 - val_acc: 0.9930\n",
      "Epoch 26/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.5006 - acc: 0.9936Epoch 00025: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 1.5007 - acc: 0.9936 - val_loss: 0.0230 - val_acc: 0.9925\n",
      "Epoch 27/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.4850 - acc: 0.9936Epoch 00026: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 1.4851 - acc: 0.9936 - val_loss: 0.0218 - val_acc: 0.9931\n",
      "Epoch 28/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.4693 - acc: 0.9940Epoch 00027: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 1.4693 - acc: 0.9940 - val_loss: 0.0220 - val_acc: 0.9930\n",
      "Epoch 29/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.4544 - acc: 0.9941Epoch 00028: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 1.4544 - acc: 0.9940 - val_loss: 0.0228 - val_acc: 0.9928\n",
      "Epoch 30/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.4391 - acc: 0.9939Epoch 00029: val_loss did not improve\n",
      "65504/65504 [==============================] - 2141s - loss: 1.4392 - acc: 0.9939 - val_loss: 0.0213 - val_acc: 0.9935\n",
      "Epoch 31/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.4239 - acc: 0.9942Epoch 00030: val_loss did not improve\n",
      "65504/65504 [==============================] - 2142s - loss: 1.4239 - acc: 0.9942 - val_loss: 0.0216 - val_acc: 0.9931\n",
      "Epoch 32/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.4095 - acc: 0.9942Epoch 00031: val_loss did not improve\n",
      "65504/65504 [==============================] - 2142s - loss: 1.4096 - acc: 0.9942 - val_loss: 0.0224 - val_acc: 0.9929\n",
      "Epoch 33/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.3942 - acc: 0.9944Epoch 00032: val_loss did not improve\n",
      "65504/65504 [==============================] - 2141s - loss: 1.3943 - acc: 0.9944 - val_loss: 0.0233 - val_acc: 0.9925\n",
      "Epoch 34/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.3801 - acc: 0.9947Epoch 00033: val_loss did not improve\n",
      "65504/65504 [==============================] - 2140s - loss: 1.3802 - acc: 0.9946 - val_loss: 0.0233 - val_acc: 0.9922\n",
      "Epoch 35/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.3658 - acc: 0.9947Epoch 00034: val_loss improved from 0.02062 to 0.02005, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2142s - loss: 1.3659 - acc: 0.9947 - val_loss: 0.0200 - val_acc: 0.9941\n",
      "Epoch 36/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.3518 - acc: 0.9946Epoch 00035: val_loss did not improve\n",
      "65504/65504 [==============================] - 2142s - loss: 1.3518 - acc: 0.9946 - val_loss: 0.0217 - val_acc: 0.9934\n",
      "Epoch 37/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.3377 - acc: 0.9946Epoch 00036: val_loss did not improve\n",
      "65504/65504 [==============================] - 2141s - loss: 1.3378 - acc: 0.9946 - val_loss: 0.0208 - val_acc: 0.9936\n",
      "Epoch 38/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.3238 - acc: 0.9949Epoch 00037: val_loss did not improve\n",
      "65504/65504 [==============================] - 2140s - loss: 1.3239 - acc: 0.9948 - val_loss: 0.0220 - val_acc: 0.9933\n",
      "Epoch 39/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.3101 - acc: 0.9947Epoch 00038: val_loss did not improve\n",
      "65504/65504 [==============================] - 2140s - loss: 1.3102 - acc: 0.9947 - val_loss: 0.0214 - val_acc: 0.9931\n",
      "Epoch 40/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.2960 - acc: 0.9948Epoch 00039: val_loss did not improve\n",
      "65504/65504 [==============================] - 2140s - loss: 1.2960 - acc: 0.9948 - val_loss: 0.0224 - val_acc: 0.9930\n",
      "Epoch 41/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.2827 - acc: 0.9951Epoch 00040: val_loss did not improve\n",
      "65504/65504 [==============================] - 2142s - loss: 1.2828 - acc: 0.9951 - val_loss: 0.0213 - val_acc: 0.9934\n",
      "Epoch 42/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.2693 - acc: 0.9950Epoch 00041: val_loss did not improve\n",
      "65504/65504 [==============================] - 2142s - loss: 1.2694 - acc: 0.9950 - val_loss: 0.0210 - val_acc: 0.9935\n",
      "Epoch 43/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.2563 - acc: 0.9949Epoch 00042: val_loss did not improve\n",
      "65504/65504 [==============================] - 2141s - loss: 1.2563 - acc: 0.9949 - val_loss: 0.0226 - val_acc: 0.9928\n",
      "Epoch 44/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.2432 - acc: 0.9952Epoch 00043: val_loss did not improve\n",
      "65504/65504 [==============================] - 2142s - loss: 1.2433 - acc: 0.9952 - val_loss: 0.0220 - val_acc: 0.9932\n",
      "Epoch 45/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.2301 - acc: 0.9952Epoch 00044: val_loss did not improve\n",
      "65504/65504 [==============================] - 2141s - loss: 1.2301 - acc: 0.9953 - val_loss: 0.0224 - val_acc: 0.9930\n",
      "Epoch 46/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.2176 - acc: 0.9954Epoch 00045: val_loss did not improve\n",
      "65504/65504 [==============================] - 2142s - loss: 1.2176 - acc: 0.9954 - val_loss: 0.0205 - val_acc: 0.9932\n",
      "Epoch 47/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.2042 - acc: 0.9956Epoch 00046: val_loss improved from 0.02005 to 0.01958, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2142s - loss: 1.2042 - acc: 0.9956 - val_loss: 0.0196 - val_acc: 0.9937\n",
      "Epoch 48/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.1925 - acc: 0.9953Epoch 00047: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.1926 - acc: 0.9953 - val_loss: 0.0208 - val_acc: 0.9932\n",
      "Epoch 49/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.1798 - acc: 0.9954Epoch 00048: val_loss improved from 0.01958 to 0.01937, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2145s - loss: 1.1798 - acc: 0.9954 - val_loss: 0.0194 - val_acc: 0.9941\n",
      "Epoch 50/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.1674 - acc: 0.9955Epoch 00049: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 1.1674 - acc: 0.9955 - val_loss: 0.0209 - val_acc: 0.9930\n",
      "Epoch 51/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.1558 - acc: 0.9955Epoch 00050: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 1.1559 - acc: 0.9955 - val_loss: 0.0205 - val_acc: 0.9935\n",
      "Epoch 52/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.1433 - acc: 0.9959Epoch 00051: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.1433 - acc: 0.9959 - val_loss: 0.0214 - val_acc: 0.9932\n",
      "Epoch 53/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.1315 - acc: 0.9958Epoch 00052: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.1315 - acc: 0.9958 - val_loss: 0.0198 - val_acc: 0.9940\n",
      "Epoch 54/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.1200 - acc: 0.9959Epoch 00053: val_loss did not improve\n",
      "65504/65504 [==============================] - 2147s - loss: 1.1200 - acc: 0.9959 - val_loss: 0.0207 - val_acc: 0.9935\n",
      "Epoch 55/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.1082 - acc: 0.9958Epoch 00054: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.1082 - acc: 0.9958 - val_loss: 0.0229 - val_acc: 0.9931\n",
      "Epoch 56/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.0967 - acc: 0.9961Epoch 00055: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.0967 - acc: 0.9960 - val_loss: 0.0201 - val_acc: 0.9940\n",
      "Epoch 57/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.0852 - acc: 0.9961Epoch 00056: val_loss did not improve\n",
      "65504/65504 [==============================] - 2147s - loss: 1.0852 - acc: 0.9961 - val_loss: 0.0208 - val_acc: 0.9940\n",
      "Epoch 58/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.0742 - acc: 0.9961Epoch 00057: val_loss did not improve\n",
      "65504/65504 [==============================] - 2147s - loss: 1.0743 - acc: 0.9961 - val_loss: 0.0210 - val_acc: 0.9933\n",
      "Epoch 59/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.0632 - acc: 0.9962Epoch 00058: val_loss improved from 0.01937 to 0.01916, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2147s - loss: 1.0632 - acc: 0.9962 - val_loss: 0.0192 - val_acc: 0.9943\n",
      "Epoch 60/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.0519 - acc: 0.9963Epoch 00059: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 1.0519 - acc: 0.9963 - val_loss: 0.0207 - val_acc: 0.9937\n",
      "Epoch 61/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.0412 - acc: 0.9963Epoch 00060: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 1.0412 - acc: 0.9963 - val_loss: 0.0204 - val_acc: 0.9941\n",
      "Epoch 62/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.0303 - acc: 0.9964Epoch 00061: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 1.0303 - acc: 0.9964 - val_loss: 0.0197 - val_acc: 0.9938\n",
      "Epoch 63/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.0198 - acc: 0.9965Epoch 00062: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 1.0199 - acc: 0.9965 - val_loss: 0.0208 - val_acc: 0.9936\n",
      "Epoch 64/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 1.0092 - acc: 0.9966Epoch 00063: val_loss did not improve\n",
      "65504/65504 [==============================] - 2149s - loss: 1.0092 - acc: 0.9966 - val_loss: 0.0200 - val_acc: 0.9940\n",
      "Epoch 65/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9992 - acc: 0.9965Epoch 00064: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 0.9992 - acc: 0.9965 - val_loss: 0.0197 - val_acc: 0.9939\n",
      "Epoch 66/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9885 - acc: 0.9965Epoch 00065: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 0.9885 - acc: 0.9965 - val_loss: 0.0204 - val_acc: 0.9940\n",
      "Epoch 67/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9784 - acc: 0.9968Epoch 00066: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 0.9785 - acc: 0.9967 - val_loss: 0.0210 - val_acc: 0.9937\n",
      "Epoch 68/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9690 - acc: 0.9966Epoch 00067: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 0.9690 - acc: 0.9966 - val_loss: 0.0202 - val_acc: 0.9940\n",
      "Epoch 69/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9591 - acc: 0.9966Epoch 00068: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 0.9591 - acc: 0.9966 - val_loss: 0.0197 - val_acc: 0.9940\n",
      "Epoch 70/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9490 - acc: 0.9968Epoch 00069: val_loss improved from 0.01916 to 0.01885, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2146s - loss: 0.9491 - acc: 0.9968 - val_loss: 0.0188 - val_acc: 0.9945\n",
      "Epoch 71/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9395 - acc: 0.9970Epoch 00070: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 0.9395 - acc: 0.9970 - val_loss: 0.0201 - val_acc: 0.9943\n",
      "Epoch 72/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9303 - acc: 0.9967Epoch 00071: val_loss improved from 0.01885 to 0.01862, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2147s - loss: 0.9303 - acc: 0.9966 - val_loss: 0.0186 - val_acc: 0.9943\n",
      "Epoch 73/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9201 - acc: 0.9969Epoch 00072: val_loss did not improve\n",
      "65504/65504 [==============================] - 2147s - loss: 0.9201 - acc: 0.9969 - val_loss: 0.0205 - val_acc: 0.9945\n",
      "Epoch 74/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9116 - acc: 0.9967Epoch 00073: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 0.9116 - acc: 0.9966 - val_loss: 0.0199 - val_acc: 0.9941\n",
      "Epoch 75/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.9015 - acc: 0.9971Epoch 00074: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 0.9016 - acc: 0.9971 - val_loss: 0.0201 - val_acc: 0.9944\n",
      "Epoch 76/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8925 - acc: 0.9973Epoch 00075: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 0.8925 - acc: 0.9973 - val_loss: 0.0204 - val_acc: 0.9941\n",
      "Epoch 77/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8842 - acc: 0.9971Epoch 00076: val_loss improved from 0.01862 to 0.01830, saving model to ./cnn_140_rgb_corrected_full_lr_0.005000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "65504/65504 [==============================] - 2145s - loss: 0.8842 - acc: 0.9971 - val_loss: 0.0183 - val_acc: 0.9944\n",
      "Epoch 78/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8748 - acc: 0.9971Epoch 00077: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 0.8748 - acc: 0.9971 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 79/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8658 - acc: 0.9974Epoch 00078: val_loss did not improve\n",
      "65504/65504 [==============================] - 2147s - loss: 0.8658 - acc: 0.9974 - val_loss: 0.0249 - val_acc: 0.9936\n",
      "Epoch 80/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8575 - acc: 0.9969Epoch 00079: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 0.8575 - acc: 0.9969 - val_loss: 0.0205 - val_acc: 0.9946\n",
      "Epoch 81/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8487 - acc: 0.9973Epoch 00080: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 0.8487 - acc: 0.9973 - val_loss: 0.0202 - val_acc: 0.9940\n",
      "Epoch 82/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8405 - acc: 0.9972Epoch 00081: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 0.8405 - acc: 0.9972 - val_loss: 0.0192 - val_acc: 0.9943\n",
      "Epoch 83/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8322 - acc: 0.9972Epoch 00082: val_loss did not improve\n",
      "65504/65504 [==============================] - 2147s - loss: 0.8322 - acc: 0.9972 - val_loss: 0.0190 - val_acc: 0.9942\n",
      "Epoch 84/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8238 - acc: 0.9973Epoch 00083: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 0.8239 - acc: 0.9973 - val_loss: 0.0196 - val_acc: 0.9945\n",
      "Epoch 85/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8160 - acc: 0.9970Epoch 00084: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 0.8160 - acc: 0.9970 - val_loss: 0.0208 - val_acc: 0.9943\n",
      "Epoch 86/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.8078 - acc: 0.9973Epoch 00085: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 0.8078 - acc: 0.9973 - val_loss: 0.0199 - val_acc: 0.9942\n",
      "Epoch 87/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7995 - acc: 0.9974Epoch 00086: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 0.7996 - acc: 0.9974 - val_loss: 0.0210 - val_acc: 0.9940\n",
      "Epoch 88/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7920 - acc: 0.9975Epoch 00087: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 0.7920 - acc: 0.9975 - val_loss: 0.0203 - val_acc: 0.9941\n",
      "Epoch 89/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7843 - acc: 0.9975Epoch 00088: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 0.7843 - acc: 0.9975 - val_loss: 0.0211 - val_acc: 0.9942\n",
      "Epoch 90/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7768 - acc: 0.9973Epoch 00089: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 0.7768 - acc: 0.9973 - val_loss: 0.0209 - val_acc: 0.9940\n",
      "Epoch 91/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7691 - acc: 0.9975Epoch 00090: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 0.7691 - acc: 0.9975 - val_loss: 0.0189 - val_acc: 0.9944\n",
      "Epoch 92/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7615 - acc: 0.9976Epoch 00091: val_loss did not improve\n",
      "65504/65504 [==============================] - 2144s - loss: 0.7615 - acc: 0.9976 - val_loss: 0.0209 - val_acc: 0.9943\n",
      "Epoch 93/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7543 - acc: 0.9974Epoch 00092: val_loss did not improve\n",
      "65504/65504 [==============================] - 2143s - loss: 0.7543 - acc: 0.9974 - val_loss: 0.0193 - val_acc: 0.9948\n",
      "Epoch 94/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7468 - acc: 0.9975Epoch 00093: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 0.7468 - acc: 0.9975 - val_loss: 0.0231 - val_acc: 0.9940\n",
      "Epoch 95/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7397 - acc: 0.9975Epoch 00094: val_loss did not improve\n",
      "65504/65504 [==============================] - 2148s - loss: 0.7397 - acc: 0.9975 - val_loss: 0.0214 - val_acc: 0.9945\n",
      "Epoch 96/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7325 - acc: 0.9975Epoch 00095: val_loss did not improve\n",
      "65504/65504 [==============================] - 2146s - loss: 0.7325 - acc: 0.9975 - val_loss: 0.0204 - val_acc: 0.9940\n",
      "Epoch 97/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7255 - acc: 0.9975Epoch 00096: val_loss did not improve\n",
      "65504/65504 [==============================] - 2145s - loss: 0.7255 - acc: 0.9975 - val_loss: 0.0202 - val_acc: 0.9944\n",
      "Epoch 98/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7187 - acc: 0.9973Epoch 00097: val_loss did not improve\n",
      "65504/65504 [==============================] - 2148s - loss: 0.7187 - acc: 0.9973 - val_loss: 0.0209 - val_acc: 0.9940\n",
      "Epoch 99/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7124 - acc: 0.9976Epoch 00098: val_loss did not improve\n",
      "65504/65504 [==============================] - 2142s - loss: 0.7124 - acc: 0.9976 - val_loss: 0.0203 - val_acc: 0.9941\n",
      "Epoch 100/100\n",
      "65472/65504 [============================>.] - ETA: 1s - loss: 0.7048 - acc: 0.9978Epoch 00099: val_loss did not improve\n",
      "65504/65504 [==============================] - 2147s - loss: 0.7048 - acc: 0.9978 - val_loss: 0.0233 - val_acc: 0.9940\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('Fitting model...')\n",
    "history = model.fit_generator(train_data_gen,\n",
    "                              nb_epoch=n_epochs,\n",
    "                              samples_per_epoch=train_data_gen.n_batches * batch_size,\n",
    "                              validation_data=validation_data_gen,\n",
    "                              nb_val_samples=validation_data_gen.n_samples,\n",
    "                              verbose=1,\n",
    "                              callbacks=[csv_logger, logger, best_model_checkpointer, current_model_checkpointer],\n",
    "                              nb_worker=2)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "done.\n",
      "Test score: 0.0248196516109\n",
      "Test accuracy: 0.992525626424\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating model...')\n",
    "score = model.evaluate_generator(test_data_gen, val_samples=test_data_gen.n_samples)\n",
    "print('done.')\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
