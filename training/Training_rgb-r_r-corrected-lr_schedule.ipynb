{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 680 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from keras.optimizers import SGD, Nadam, RMSprop\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "import config\n",
    "\n",
    "from utils.dataset.data_generator import DataGenerator\n",
    "from models.cnn3 import cnn, cnn_regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr=0.1\n",
    "l1 = 0.00001\n",
    "l2 = 0.00001\n",
    "dropout = 0.5\n",
    "n_epochs=500\n",
    "batch_size=32\n",
    "input_shape=(140, 140, 3)\n",
    "\n",
    "name = 'cnn_140_rgb_corrected_lr_scheduled_5_%f_sgd_he_normal__l1_%f_l2_%f_dropout_%f_r' % (lr, l1, l2, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(i):\n",
    "    global lr\n",
    "    if i > 0:\n",
    "        return lr / 2 ** (i / 5)\n",
    "    \n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 140, 140, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 140, 140, 128) 18944       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 140, 140, 128) 0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 70, 70, 128)   0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 70, 70, 64)    204864      maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 70, 70, 64)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 35, 35, 64)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 35, 35, 64)    36928       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 35, 35, 64)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 17, 17, 64)    0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 18496)         0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          18940928    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 1024)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1024)          1049600     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 1024)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 512)           524800      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             1026        dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 2)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 20777090\n",
      "____________________________________________________________________________________________________\n",
      "compiling model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('loading model...')\n",
    "# model = cnn(input_shape=input_shape, init='he_normal')\n",
    "model = cnn_regularized(input_shape=input_shape, init='he_normal', l1=l1, l2=l2)\n",
    "model.summary()\n",
    "\n",
    "optimizer = SGD(lr=lr, clipnorm=4., nesterov=True)\n",
    "# optimizer = Nadam(lr=lr)\n",
    "# optimizer = RMSprop(lr=lr)\n",
    "\n",
    "print('compiling model...')\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print('done.')\n",
    "\n",
    "csv_logger = CSVLogger('%s_training.log' % name)\n",
    "best_model_checkpointer = ModelCheckpoint(filepath=(\"./%s_training_weights_best.hdf5\" % name), verbose=1,\n",
    "                                          save_best_only=True)\n",
    "\n",
    "current_model_checkpointer = ModelCheckpoint(filepath=(\"./%s_training_weights_current.hdf5\" % name), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data generators...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('Initializing data generators...')\n",
    "train_set_file = '/home/tanuj/Workspace/power-grid-detection/dataset/raw/19/train_data.csv'\n",
    "validation_set_file = '/home/tanuj/Workspace/power-grid-detection/dataset/raw/19/validation_data.csv'\n",
    "test_set_file = '/home/tanuj/Workspace/power-grid-detection/dataset/raw/19/test_data.csv'\n",
    "\n",
    "train_data_gen = DataGenerator(dataset_file=train_set_file, batch_size=batch_size)\n",
    "validation_data_gen = DataGenerator(dataset_file=validation_set_file, batch_size=batch_size)\n",
    "test_data_gen = DataGenerator(dataset_file=test_set_file, batch_size=batch_size)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Epoch 1/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.8025 - acc: 0.6536Epoch 00000: val_loss improved from inf to 0.55571, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 353s - loss: 2.8027 - acc: 0.6533 - val_loss: 0.5557 - val_acc: 0.6981\n",
      "Epoch 2/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.7490 - acc: 0.5974Epoch 00001: val_loss did not improve\n",
      "10496/10496 [==============================] - 353s - loss: 2.7490 - acc: 0.5971 - val_loss: 0.6930 - val_acc: 0.5002\n",
      "Epoch 3/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.6833 - acc: 0.5742Epoch 00002: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 2.6832 - acc: 0.5743 - val_loss: 0.6024 - val_acc: 0.7386\n",
      "Epoch 4/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.5548 - acc: 0.6398Epoch 00003: val_loss improved from 0.55571 to 0.48818, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 354s - loss: 2.5546 - acc: 0.6399 - val_loss: 0.4882 - val_acc: 0.7568\n",
      "Epoch 5/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.3742 - acc: 0.7363Epoch 00004: val_loss improved from 0.48818 to 0.40262, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 354s - loss: 2.3740 - acc: 0.7362 - val_loss: 0.4026 - val_acc: 0.7733\n",
      "Epoch 6/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.2558 - acc: 0.7689Epoch 00005: val_loss improved from 0.40262 to 0.40043, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 355s - loss: 2.2560 - acc: 0.7688 - val_loss: 0.4004 - val_acc: 0.7751\n",
      "Epoch 7/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.2208 - acc: 0.7694Epoch 00006: val_loss improved from 0.40043 to 0.38950, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 355s - loss: 2.2208 - acc: 0.7693 - val_loss: 0.3895 - val_acc: 0.7784\n",
      "Epoch 8/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.1846 - acc: 0.7703Epoch 00007: val_loss did not improve\n",
      "10496/10496 [==============================] - 353s - loss: 2.1846 - acc: 0.7701 - val_loss: 0.4035 - val_acc: 0.7628\n",
      "Epoch 9/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.1485 - acc: 0.7751Epoch 00008: val_loss improved from 0.38950 to 0.38898, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 354s - loss: 2.1486 - acc: 0.7751 - val_loss: 0.3890 - val_acc: 0.7815\n",
      "Epoch 10/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.1138 - acc: 0.7773Epoch 00009: val_loss did not improve\n",
      "10496/10496 [==============================] - 352s - loss: 2.1139 - acc: 0.7771 - val_loss: 0.3892 - val_acc: 0.7740\n",
      "Epoch 11/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0822 - acc: 0.7811Epoch 00010: val_loss improved from 0.38898 to 0.38295, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 354s - loss: 2.0822 - acc: 0.7810 - val_loss: 0.3829 - val_acc: 0.7828\n",
      "Epoch 12/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0627 - acc: 0.7833Epoch 00011: val_loss improved from 0.38295 to 0.38212, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 353s - loss: 2.0627 - acc: 0.7832 - val_loss: 0.3821 - val_acc: 0.7775\n",
      "Epoch 13/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0464 - acc: 0.7834Epoch 00012: val_loss improved from 0.38212 to 0.38068, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 353s - loss: 2.0464 - acc: 0.7833 - val_loss: 0.3807 - val_acc: 0.7799\n",
      "Epoch 14/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0299 - acc: 0.7837Epoch 00013: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 2.0299 - acc: 0.7836 - val_loss: 0.3813 - val_acc: 0.7790\n",
      "Epoch 15/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 2.0144 - acc: 0.7843Epoch 00014: val_loss improved from 0.38068 to 0.37948, saving model to ./cnn_140_rgb_corrected_lr_scheduled_5_0.100000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 355s - loss: 2.0144 - acc: 0.7842 - val_loss: 0.3795 - val_acc: 0.7821\n",
      "Epoch 16/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9975 - acc: 0.7865Epoch 00015: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9975 - acc: 0.7865 - val_loss: 0.3797 - val_acc: 0.7826\n",
      "Epoch 17/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9876 - acc: 0.7876Epoch 00016: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9876 - acc: 0.7876 - val_loss: 0.3820 - val_acc: 0.7806\n",
      "Epoch 18/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9791 - acc: 0.7885Epoch 00017: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9792 - acc: 0.7884 - val_loss: 0.3825 - val_acc: 0.7766\n",
      "Epoch 19/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9696 - acc: 0.7895Epoch 00018: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9696 - acc: 0.7893 - val_loss: 0.3802 - val_acc: 0.7821\n",
      "Epoch 20/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9604 - acc: 0.7899Epoch 00019: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9603 - acc: 0.7899 - val_loss: 0.3804 - val_acc: 0.7839\n",
      "Epoch 21/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9527 - acc: 0.7907Epoch 00020: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9527 - acc: 0.7906 - val_loss: 0.3815 - val_acc: 0.7826\n",
      "Epoch 22/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9472 - acc: 0.7911Epoch 00021: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9472 - acc: 0.7911 - val_loss: 0.3809 - val_acc: 0.7819\n",
      "Epoch 23/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9429 - acc: 0.7916Epoch 00022: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9428 - acc: 0.7914 - val_loss: 0.3828 - val_acc: 0.7801\n",
      "Epoch 24/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9377 - acc: 0.7922Epoch 00023: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9377 - acc: 0.7921 - val_loss: 0.3814 - val_acc: 0.7850\n",
      "Epoch 25/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9334 - acc: 0.7923Epoch 00024: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9335 - acc: 0.7922 - val_loss: 0.3833 - val_acc: 0.7850\n",
      "Epoch 26/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9280 - acc: 0.7937Epoch 00025: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9280 - acc: 0.7937 - val_loss: 0.3839 - val_acc: 0.7835\n",
      "Epoch 27/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9268 - acc: 0.7940Epoch 00026: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9267 - acc: 0.7940 - val_loss: 0.3850 - val_acc: 0.7841\n",
      "Epoch 28/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9226 - acc: 0.7947Epoch 00027: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9226 - acc: 0.7946 - val_loss: 0.3917 - val_acc: 0.7821\n",
      "Epoch 29/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9215 - acc: 0.7938Epoch 00028: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9215 - acc: 0.7936 - val_loss: 0.3900 - val_acc: 0.7821\n",
      "Epoch 30/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9171 - acc: 0.7948Epoch 00029: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9170 - acc: 0.7947 - val_loss: 0.3877 - val_acc: 0.7839\n",
      "Epoch 31/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9167 - acc: 0.7954Epoch 00030: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9168 - acc: 0.7954 - val_loss: 0.3900 - val_acc: 0.7828\n",
      "Epoch 32/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9155 - acc: 0.7949Epoch 00031: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9155 - acc: 0.7949 - val_loss: 0.3901 - val_acc: 0.7821\n",
      "Epoch 33/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9135 - acc: 0.7946Epoch 00032: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9135 - acc: 0.7945 - val_loss: 0.3876 - val_acc: 0.7828\n",
      "Epoch 34/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9127 - acc: 0.7952Epoch 00033: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9127 - acc: 0.7951 - val_loss: 0.3899 - val_acc: 0.7826\n",
      "Epoch 35/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9099 - acc: 0.7963Epoch 00034: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9099 - acc: 0.7961 - val_loss: 0.3886 - val_acc: 0.7837\n",
      "Epoch 36/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9095 - acc: 0.7955Epoch 00035: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9095 - acc: 0.7954 - val_loss: 0.3892 - val_acc: 0.7852\n",
      "Epoch 37/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9091 - acc: 0.7967Epoch 00036: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9092 - acc: 0.7966 - val_loss: 0.3942 - val_acc: 0.7830\n",
      "Epoch 38/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9086 - acc: 0.7963Epoch 00037: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9088 - acc: 0.7961 - val_loss: 0.3903 - val_acc: 0.7850\n",
      "Epoch 39/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9090 - acc: 0.7959Epoch 00038: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9091 - acc: 0.7957 - val_loss: 0.3910 - val_acc: 0.7841\n",
      "Epoch 40/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9057 - acc: 0.7965Epoch 00039: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9058 - acc: 0.7963 - val_loss: 0.3883 - val_acc: 0.7830\n",
      "Epoch 41/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9057 - acc: 0.7965Epoch 00040: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9057 - acc: 0.7964 - val_loss: 0.3942 - val_acc: 0.7828\n",
      "Epoch 42/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9044 - acc: 0.7965Epoch 00041: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9044 - acc: 0.7964 - val_loss: 0.3918 - val_acc: 0.7837\n",
      "Epoch 43/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9071 - acc: 0.7956Epoch 00042: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9071 - acc: 0.7955 - val_loss: 0.3891 - val_acc: 0.7841\n",
      "Epoch 44/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9047 - acc: 0.7967Epoch 00043: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9047 - acc: 0.7966 - val_loss: 0.3895 - val_acc: 0.7848\n",
      "Epoch 45/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9068 - acc: 0.7955Epoch 00044: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9068 - acc: 0.7954 - val_loss: 0.3944 - val_acc: 0.7817\n",
      "Epoch 46/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9052 - acc: 0.7963Epoch 00045: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9053 - acc: 0.7962 - val_loss: 0.3897 - val_acc: 0.7846\n",
      "Epoch 47/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9054 - acc: 0.7955Epoch 00046: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9054 - acc: 0.7954 - val_loss: 0.3891 - val_acc: 0.7852\n",
      "Epoch 48/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9044 - acc: 0.7967Epoch 00047: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9044 - acc: 0.7966 - val_loss: 0.3981 - val_acc: 0.7821\n",
      "Epoch 49/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9041 - acc: 0.7962Epoch 00048: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9041 - acc: 0.7960 - val_loss: 0.3897 - val_acc: 0.7850\n",
      "Epoch 50/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9036 - acc: 0.7960Epoch 00049: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9037 - acc: 0.7958 - val_loss: 0.3902 - val_acc: 0.7846\n",
      "Epoch 51/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9038 - acc: 0.7974Epoch 00050: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9039 - acc: 0.7972 - val_loss: 0.3968 - val_acc: 0.7817\n",
      "Epoch 52/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9041 - acc: 0.7967Epoch 00051: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9041 - acc: 0.7966 - val_loss: 0.3939 - val_acc: 0.7835\n",
      "Epoch 53/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9032 - acc: 0.7963Epoch 00052: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9032 - acc: 0.7963 - val_loss: 0.3904 - val_acc: 0.7848\n",
      "Epoch 54/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9041 - acc: 0.7970Epoch 00053: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9042 - acc: 0.7968 - val_loss: 0.3934 - val_acc: 0.7839\n",
      "Epoch 55/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9028 - acc: 0.7975Epoch 00054: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9028 - acc: 0.7974 - val_loss: 0.3941 - val_acc: 0.7830\n",
      "Epoch 56/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9045 - acc: 0.7968Epoch 00055: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9045 - acc: 0.7967 - val_loss: 0.3925 - val_acc: 0.7846\n",
      "Epoch 57/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9027 - acc: 0.7961Epoch 00056: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9028 - acc: 0.7960 - val_loss: 0.3916 - val_acc: 0.7850\n",
      "Epoch 58/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9029 - acc: 0.7963Epoch 00057: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9029 - acc: 0.7962 - val_loss: 0.3943 - val_acc: 0.7835\n",
      "Epoch 59/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9027 - acc: 0.7961Epoch 00058: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9027 - acc: 0.7960 - val_loss: 0.3931 - val_acc: 0.7835\n",
      "Epoch 60/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9025 - acc: 0.7976Epoch 00059: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9025 - acc: 0.7975 - val_loss: 0.3939 - val_acc: 0.7839\n",
      "Epoch 61/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9025 - acc: 0.7968Epoch 00060: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9026 - acc: 0.7967 - val_loss: 0.3942 - val_acc: 0.7830\n",
      "Epoch 62/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9032 - acc: 0.7971Epoch 00061: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9033 - acc: 0.7970 - val_loss: 0.3926 - val_acc: 0.7846\n",
      "Epoch 63/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9036 - acc: 0.7961Epoch 00062: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9036 - acc: 0.7959 - val_loss: 0.3930 - val_acc: 0.7828\n",
      "Epoch 64/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9016 - acc: 0.7971Epoch 00063: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9017 - acc: 0.7969 - val_loss: 0.3922 - val_acc: 0.7839\n",
      "Epoch 65/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9035 - acc: 0.7959Epoch 00064: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9035 - acc: 0.7958 - val_loss: 0.3904 - val_acc: 0.7848\n",
      "Epoch 66/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9034 - acc: 0.7969Epoch 00065: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9035 - acc: 0.7967 - val_loss: 0.3932 - val_acc: 0.7830\n",
      "Epoch 67/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9028 - acc: 0.7956Epoch 00066: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9028 - acc: 0.7955 - val_loss: 0.3925 - val_acc: 0.7830\n",
      "Epoch 68/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9032 - acc: 0.7958Epoch 00067: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9032 - acc: 0.7957 - val_loss: 0.3935 - val_acc: 0.7826\n",
      "Epoch 69/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9035 - acc: 0.7968Epoch 00068: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9035 - acc: 0.7967 - val_loss: 0.3919 - val_acc: 0.7839\n",
      "Epoch 70/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9018 - acc: 0.7969Epoch 00069: val_loss did not improve\n",
      "10496/10496 [==============================] - 357s - loss: 1.9018 - acc: 0.7969 - val_loss: 0.3956 - val_acc: 0.7826\n",
      "Epoch 71/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9031 - acc: 0.7966Epoch 00070: val_loss did not improve\n",
      "10496/10496 [==============================] - 356s - loss: 1.9034 - acc: 0.7964 - val_loss: 0.3966 - val_acc: 0.7837\n",
      "Epoch 72/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9037 - acc: 0.7973Epoch 00071: val_loss did not improve\n",
      "10496/10496 [==============================] - 355s - loss: 1.9037 - acc: 0.7972 - val_loss: 0.3962 - val_acc: 0.7832\n",
      "Epoch 73/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 1.9036 - acc: 0.7959Epoch 00072: val_loss did not improve\n",
      "10496/10496 [==============================] - 354s - loss: 1.9036 - acc: 0.7957 - val_loss: 0.3916 - val_acc: 0.7841\n",
      "Epoch 74/500\n",
      " 6880/10496 [==================>...........] - ETA: 117s - loss: 1.9017 - acc: 0.7997"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e658bc878b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_checkpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_model_checkpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                               nb_worker=2)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanuj/.tools/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1460\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1462\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanuj/.tools/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanuj/.tools/anaconda3/envs/py27/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tanuj/.tools/anaconda3/envs/py27/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Fitting model...')\n",
    "history = model.fit_generator(train_data_gen,\n",
    "                              nb_epoch=n_epochs,\n",
    "                              samples_per_epoch=train_data_gen.n_batches * batch_size,\n",
    "                              validation_data=validation_data_gen,\n",
    "                              nb_val_samples=validation_data_gen.n_samples,\n",
    "                              verbose=1,\n",
    "                              callbacks=[csv_logger, best_model_checkpointer, current_model_checkpointer, lr_scheduler],\n",
    "                              nb_worker=2)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Evaluating model...')\n",
    "score = model.evaluate_generator(test_data_gen, val_samples=test_data_gen.n_samples)\n",
    "print('done.')\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
