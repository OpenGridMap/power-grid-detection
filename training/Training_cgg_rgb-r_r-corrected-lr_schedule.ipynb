{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "DEBUG: nvcc STDOUT nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "mod.cu\n",
      "   Creating library C:/Users/Tanuj/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.12-64/tmp0z1as8/265abc51f7c376c224983485238ff1a5.lib and object C:/Users/Tanuj/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.12-64/tmp0z1as8/265abc51f7c376c224983485238ff1a5.exp\n",
      "\n",
      "Using gpu device 0: GeForce GTX 970M (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n",
      "C:\\Users\\Tanuj\\Anaconda2\\lib\\site-packages\\theano\\sandbox\\cuda\\__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from keras.optimizers import SGD, Nadam, RMSprop\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "import config\n",
    "\n",
    "from utils.dataset.data_generator import DataGenerator\n",
    "from models.vgg16 import vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "l1 = 0.00001\n",
    "l2 = 0.00001\n",
    "dropout = 0.5\n",
    "n_epochs=500\n",
    "batch_size=32\n",
    "input_shape=(140, 140, 3)\n",
    "\n",
    "name = 'vgg_140_rgb_corrected_lr_%f_sgd_he_normal__l1_%f_l2_%f_dropout_%f_r' % (lr, l1, l2, dropout)\n",
    "# scheduled_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(i):\n",
    "    global lr\n",
    "    if i > 0:\n",
    "        return lr / 2 ** (i / 5)\n",
    "    \n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 140, 140, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, 140, 140, 64)  1792        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 140, 140, 64)  0           block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, 140, 140, 64)  36928       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 140, 140, 64)  0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 70, 70, 64)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, 70, 70, 128)   73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 70, 70, 128)   0           block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, 70, 70, 128)   147584      activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 70, 70, 128)   0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 35, 35, 128)   0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, 35, 35, 256)   295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 35, 35, 256)   0           block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, 35, 35, 256)   590080      activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 35, 35, 256)   0           block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, 35, 35, 256)   590080      activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 35, 35, 256)   0           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 17, 17, 256)   0           activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, 17, 17, 512)   1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 17, 17, 512)   0           block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, 17, 17, 512)   2359808     activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 17, 17, 512)   0           block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, 17, 17, 512)   2359808     activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 17, 17, 512)   0           block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, 8, 8, 512)     0           activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, 8, 8, 512)     2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 8, 8, 512)     0           block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, 8, 8, 512)     2359808     activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 8, 8, 512)     0           block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, 8, 8, 512)     2359808     activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 8, 8, 512)     0           block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, 4, 4, 512)     0           activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 8192)          0           block5_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "fc1 (Dense)                      (None, 4096)          33558528    flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 4096)          0           fc1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc2 (Dense)                      (None, 4096)          16781312    activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 4096)          0           fc2[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 2)             8194        activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 2)             0           predictions[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 65,062,722\n",
      "Trainable params: 65,062,722\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "compiling model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('loading model...')\n",
    "# model = cnn(input_shape=input_shape, init='he_normal')\n",
    "model = vgg(input_shape=input_shape)\n",
    "model.summary()\n",
    "\n",
    "optimizer = SGD(lr=lr, clipnorm=4., nesterov=True)\n",
    "# optimizer = Nadam(lr=lr)\n",
    "# optimizer = RMSprop(lr=lr)\n",
    "\n",
    "print('compiling model...')\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print('done.')\n",
    "\n",
    "csv_logger = CSVLogger('%s_training.log' % name)\n",
    "best_model_checkpointer = ModelCheckpoint(filepath=(\"./%s_training_weights_best.hdf5\" % name), verbose=1,\n",
    "                                          save_best_only=True)\n",
    "\n",
    "current_model_checkpointer = ModelCheckpoint(filepath=(\"./%s_training_weights_current.hdf5\" % name), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data generators...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('Initializing data generators...')\n",
    "\n",
    "train_set_file = config.corrected_train_data_file\n",
    "validation_set_file = config.corrected_validation_data_file\n",
    "test_set_file = config.corrected_test_data_file\n",
    "# train_set_file = os.path.join(config.dataset_dir, 'corrected', '19', 'train_data.csv')\n",
    "# validation_set_file = os.path.join(config.dataset_dir, 'corrected', '19', 'validation_data.csv')\n",
    "# test_set_file = os.path.join(config.dataset_dir, 'corrected', '19', 'test_data.csv')\n",
    "\n",
    "train_data_gen = DataGenerator(dataset_file=train_set_file, batch_size=batch_size)\n",
    "validation_data_gen = DataGenerator(dataset_file=validation_set_file, batch_size=batch_size)\n",
    "test_data_gen = DataGenerator(dataset_file=test_set_file, batch_size=batch_size)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Epoch 1/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6764 - acc: 0.6644Epoch 00000: val_loss improved from inf to 0.66146, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 507s - loss: 0.6765 - acc: 0.6641 - val_loss: 0.6615 - val_acc: 0.6734\n",
      "Epoch 2/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6554 - acc: 0.6650Epoch 00001: val_loss improved from 0.66146 to 0.64685, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6555 - acc: 0.6647 - val_loss: 0.6468 - val_acc: 0.6717\n",
      "Epoch 3/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6457 - acc: 0.6650Epoch 00002: val_loss improved from 0.64685 to 0.63909, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6458 - acc: 0.6647 - val_loss: 0.6391 - val_acc: 0.6730\n",
      "Epoch 4/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6412 - acc: 0.6650Epoch 00003: val_loss improved from 0.63909 to 0.63562, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6414 - acc: 0.6647 - val_loss: 0.6356 - val_acc: 0.6730\n",
      "Epoch 5/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6392 - acc: 0.6650Epoch 00004: val_loss improved from 0.63562 to 0.63292, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6394 - acc: 0.6647 - val_loss: 0.6329 - val_acc: 0.6747\n",
      "Epoch 6/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6383 - acc: 0.6650Epoch 00005: val_loss improved from 0.63292 to 0.63233, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 507s - loss: 0.6385 - acc: 0.6647 - val_loss: 0.6323 - val_acc: 0.6743\n",
      "Epoch 7/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6380 - acc: 0.6650Epoch 00006: val_loss improved from 0.63233 to 0.63190, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6382 - acc: 0.6647 - val_loss: 0.6319 - val_acc: 0.6743\n",
      "Epoch 8/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6378 - acc: 0.6650Epoch 00007: val_loss did not improve\n",
      "10496/10496 [==============================] - 504s - loss: 0.6380 - acc: 0.6647 - val_loss: 0.6331 - val_acc: 0.6721\n",
      "Epoch 9/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00008: val_loss improved from 0.63190 to 0.63183, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6318 - val_acc: 0.6739\n",
      "Epoch 10/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00009: val_loss did not improve\n",
      "10496/10496 [==============================] - 503s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6321 - val_acc: 0.6734\n",
      "Epoch 11/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00010: val_loss did not improve\n",
      "10496/10496 [==============================] - 505s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6320 - val_acc: 0.6734\n",
      "Epoch 12/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00011: val_loss did not improve\n",
      "10496/10496 [==============================] - 504s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6320 - val_acc: 0.6734\n",
      "Epoch 13/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00012: val_loss improved from 0.63183 to 0.63138, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6314 - val_acc: 0.6743\n",
      "Epoch 14/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00013: val_loss improved from 0.63138 to 0.63137, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6314 - val_acc: 0.6743\n",
      "Epoch 15/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00014: val_loss did not improve\n",
      "10496/10496 [==============================] - 504s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6337 - val_acc: 0.6708\n",
      "Epoch 16/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00015: val_loss improved from 0.63137 to 0.63136, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6314 - val_acc: 0.6743\n",
      "Epoch 17/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00016: val_loss did not improve\n",
      "10496/10496 [==============================] - 505s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6320 - val_acc: 0.6734\n",
      "Epoch 18/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00017: val_loss did not improve\n",
      "10496/10496 [==============================] - 505s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6319 - val_acc: 0.6734\n",
      "Epoch 19/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00018: val_loss did not improve\n",
      "10496/10496 [==============================] - 504s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6325 - val_acc: 0.6725\n",
      "Epoch 20/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00019: val_loss improved from 0.63136 to 0.63135, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6313 - val_acc: 0.6743\n",
      "Epoch 21/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00020: val_loss did not improve\n",
      "10496/10496 [==============================] - 504s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6337 - val_acc: 0.6708\n",
      "Epoch 22/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00021: val_loss did not improve\n",
      "10496/10496 [==============================] - 504s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6319 - val_acc: 0.6734\n",
      "Epoch 23/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00022: val_loss did not improve\n",
      "10496/10496 [==============================] - 503s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6325 - val_acc: 0.6725\n",
      "Epoch 24/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00023: val_loss did not improve\n",
      "10496/10496 [==============================] - 503s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6316 - val_acc: 0.6739\n",
      "Epoch 25/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00024: val_loss did not improve\n",
      "10496/10496 [==============================] - 505s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6325 - val_acc: 0.6725\n",
      "Epoch 26/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00025: val_loss improved from 0.63135 to 0.63135, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 509s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6313 - val_acc: 0.6743\n",
      "Epoch 27/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00026: val_loss did not improve\n",
      "10496/10496 [==============================] - 503s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6337 - val_acc: 0.6708\n",
      "Epoch 28/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00027: val_loss improved from 0.63135 to 0.63135, saving model to ./vgg_140_rgb_corrected_lr_0.001000_sgd_he_normal__l1_0.000010_l2_0.000010_dropout_0.500000_r_training_weights_best.hdf5\n",
      "10496/10496 [==============================] - 508s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6313 - val_acc: 0.6743\n",
      "Epoch 29/500\n",
      "10464/10496 [============================>.] - ETA: 1s - loss: 0.6377 - acc: 0.6650Epoch 00028: val_loss did not improve\n",
      "10496/10496 [==============================] - 504s - loss: 0.6379 - acc: 0.6647 - val_loss: 0.6331 - val_acc: 0.6717\n",
      "Epoch 30/500\n",
      " 7424/10496 [====================>.........] - ETA: 142s - loss: 0.6347 - acc: 0.6693"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f3a96eb8f3e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model_checkpointer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_model_checkpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                               nb_worker=2)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'done.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# , lr_scheduler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tanuj\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1506\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1507\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1508\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1509\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tanuj\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1265\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tanuj\\Anaconda2\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tanuj\\Anaconda2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Fitting model...')\n",
    "history = model.fit_generator(train_data_gen,\n",
    "                              nb_epoch=n_epochs,\n",
    "                              samples_per_epoch=train_data_gen.n_batches * batch_size,\n",
    "                              validation_data=validation_data_gen,\n",
    "                              nb_val_samples=validation_data_gen.n_samples,\n",
    "                              verbose=1,\n",
    "                              callbacks=[csv_logger, best_model_checkpointer, current_model_checkpointer],\n",
    "                              nb_worker=2)\n",
    "print('done.')\n",
    "# , lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "done.\n",
      "Test score: 0.634301039535\n",
      "Test accuracy: 0.669894366197\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating model...')\n",
    "score = model.evaluate_generator(test_data_gen, val_samples=test_data_gen.n_samples)\n",
    "print('done.')\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
